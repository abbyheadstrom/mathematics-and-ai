{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3370b4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:37:53.735431Z",
     "start_time": "2024-07-24T14:37:53.709857Z"
    }
   },
   "outputs": [],
   "source": [
    "# Homework 4 (due 07/24/2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58735d16-cb06-47ba-ac58-c608b9d06f78",
   "metadata": {},
   "source": [
    "# Decision trees, interpretability, and algorithmic bias\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this week's project, you will explore the COMPAS data set. COMPAS stands for \"Correctional Offender Management Profiling for Alternative Sanctions\". It is a software/algorithm that is used to assess the risk of a registered offender is going to commit another offense. Although researchers and journalists have pointed to [various problems](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) of this algorithm over many years, the algorithm is still used to inform sentences and parole decisions in several US states. \n",
    "You can learn more about the COMPAS data set [here](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis). \n",
    "\n",
    "Through this project, you will practice fitting and validating several classification models and you will explore some distinct benefits of using decision trees in machine learning. As part of that exploration, you are going to audit your model for demographic biases via a \"closed box\" and an \"open box\" approach.\n",
    "\n",
    "The COMPAS data set is a favorite example among critics of machine learning because it demonstrates several shortcomings and failure modes of machine learning techniques. The lessons learned from this project might be discouraging, and they are important. Keep in mind, however, that what you see here does not generalize to all data sets. \n",
    "\n",
    "This project has four parts.\n",
    "\n",
    "### Part 1: Prepare the COMPAS data set  (PARTIALLY YOU TO COMPLETE)\n",
    "\n",
    "In this part, you will load the COMPAS data set, explore its content, and select several variables as features (i.e., queries) or class labels (i.e., responses). Some of these features are not numerical, so you will need to replace some categorical values with zeros and ones. Your features will include categorical variable with more than two categories. You will uses 1-hot encoding to include this feature in your data set. \n",
    "\n",
    "This part includes four steps:\n",
    "1. Load and explore data set\n",
    "2. Select features and response variables\n",
    "3. Construct numerical coding for categorical features\n",
    "4. Split the data\n",
    "\n",
    "### Part 2: Train and validate a decision tree  (PARTIALLY YOU TO COMPLETE)\n",
    "\n",
    "In this part, you will fit a decision tree to your data. You will examine the effect of tuning the complexity of the tree via the \"maximum number of leaves\" parameter and use 5-fold cross-validation to find an optimal value.\n",
    "\n",
    "This part includes three steps:\n",
    "\n",
    "1. Fit a decision tree on the training data\n",
    "2. Tune the parameter \"maximum number of leaves\"\n",
    "3. Calculate the selected model's test performance\n",
    "\n",
    "\n",
    "### Part 3: Auditing a decision tree for demographic biases  (PARTIALLY YOU TO COMPLETE)\n",
    "\n",
    "Your training data includes several demographic variables (i.e., age, sex, race). A crude way to assess whether a model has some demographic bias is to remove the corresponding variables from your training data and explore how that removal affects your model's performance. Decision trees have the advantage of being interpretable machine learning models. By going through the decision nodes (i.e., branching points), you can \"open the black box and look inside\". Specifically, you can assess how each feature is used in the decision making process.\n",
    "\n",
    "This part includes three steps:\n",
    "\n",
    "1. Fit a decision tree\n",
    "2. Check for racial bias via performance assessment\n",
    "3. Check for racial bias via decision rules\n",
    "\n",
    "### Part 4: Comparison to other linear classifiers (FOR YOU TO COMPLETE)\n",
    "\n",
    "For some types of data, decision trees tend to achieve lower prediction accuracies In this part, you will train and tune several classifiers on the COMPAS data. You will then compare their performance on your test set.\n",
    "\n",
    "This part includes three steps:\n",
    "\n",
    "1. Fit LDA and logistic regression\n",
    "2. Tune and fit ensemble methods\n",
    "3. Tune and fit SVC\n",
    "4. Compare performance metrics for all models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf61d6ee-6086-420a-a1b3-9002b2d292b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:37:55.216079Z",
     "start_time": "2024-07-24T14:37:53.732094Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b14b03-f395-4bb0-abce-f67293c4a5db",
   "metadata": {},
   "source": [
    "## Part 1: Prepare the COMPAS data set\n",
    "\n",
    ">In this part, you will load the COMPAS data set, explore its content, and select several variables as features (i.e., queries) or class labels (i.e., responses). Some of these features are not numerical, so you will need to replace some categorical values with zeros and ones. Your features will include categorical variable with more than two categories. You will uses 1-hot encoding to include this feature in your data set.\n",
    ">\n",
    ">This part includes four steps:\n",
    ">1. Load and explore data set\n",
    ">2. Select features and response variables\n",
    ">3. Construct numerical coding for categorical features\n",
    ">4. Split the data\n",
    "\n",
    "\n",
    "\n",
    "### Part 1, Step 1: Load and explore data set\n",
    "\n",
    "This folder includes the 'compas-scores-two-years.csv' file. The COMPAS data that you will use for this project is in this file. It is always a good idea to look at the raw data before proceeding with one's machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f35e4e7-8eab-44b2-a03f-5ed100f53516",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:37:55.279509Z",
     "start_time": "2024-07-24T14:37:55.216913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'name', 'first', 'last', 'compas_screening_date', 'sex', 'dob',\n",
      "       'age', 'age_cat', 'race', 'juv_fel_count', 'decile_score',\n",
      "       'juv_misd_count', 'juv_other_count', 'priors_count',\n",
      "       'days_b_screening_arrest', 'c_jail_in', 'c_jail_out', 'c_case_number',\n",
      "       'c_offense_date', 'c_arrest_date', 'c_days_from_compas',\n",
      "       'c_charge_degree', 'c_charge_desc', 'is_recid', 'r_case_number',\n",
      "       'r_charge_degree', 'r_days_from_arrest', 'r_offense_date',\n",
      "       'r_charge_desc', 'r_jail_in', 'r_jail_out', 'violent_recid',\n",
      "       'is_violent_recid', 'vr_case_number', 'vr_charge_degree',\n",
      "       'vr_offense_date', 'vr_charge_desc', 'type_of_assessment',\n",
      "       'decile_score.1', 'score_text', 'screening_date',\n",
      "       'v_type_of_assessment', 'v_decile_score', 'v_score_text',\n",
      "       'v_screening_date', 'in_custody', 'out_custody', 'priors_count.1',\n",
      "       'start', 'end', 'event', 'two_year_recid'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": "   id                name   first         last compas_screening_date   sex  \\\n0   1    miguel hernandez  miguel    hernandez            2013-08-14  Male   \n1   3         kevon dixon   kevon        dixon            2013-01-27  Male   \n2   4            ed philo      ed        philo            2013-04-14  Male   \n3   5         marcu brown   marcu        brown            2013-01-13  Male   \n4   6  bouthy pierrelouis  bouthy  pierrelouis            2013-03-26  Male   \n\n          dob  age          age_cat              race  ...  v_decile_score  \\\n0  1947-04-18   69  Greater than 45             Other  ...               1   \n1  1982-01-22   34          25 - 45  African-American  ...               1   \n2  1991-05-14   24     Less than 25  African-American  ...               3   \n3  1993-01-21   23     Less than 25  African-American  ...               6   \n4  1973-01-22   43          25 - 45             Other  ...               1   \n\n   v_score_text  v_screening_date  in_custody  out_custody  priors_count.1  \\\n0           Low        2013-08-14  2014-07-07   2014-07-14               0   \n1           Low        2013-01-27  2013-01-26   2013-02-05               0   \n2           Low        2013-04-14  2013-06-16   2013-06-16               4   \n3        Medium        2013-01-13         NaN          NaN               1   \n4           Low        2013-03-26         NaN          NaN               2   \n\n  start   end event two_year_recid  \n0     0   327     0              0  \n1     9   159     1              1  \n2     0    63     0              1  \n3     0  1174     0              0  \n4     0  1102     0              0  \n\n[5 rows x 53 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>name</th>\n      <th>first</th>\n      <th>last</th>\n      <th>compas_screening_date</th>\n      <th>sex</th>\n      <th>dob</th>\n      <th>age</th>\n      <th>age_cat</th>\n      <th>race</th>\n      <th>...</th>\n      <th>v_decile_score</th>\n      <th>v_score_text</th>\n      <th>v_screening_date</th>\n      <th>in_custody</th>\n      <th>out_custody</th>\n      <th>priors_count.1</th>\n      <th>start</th>\n      <th>end</th>\n      <th>event</th>\n      <th>two_year_recid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>miguel hernandez</td>\n      <td>miguel</td>\n      <td>hernandez</td>\n      <td>2013-08-14</td>\n      <td>Male</td>\n      <td>1947-04-18</td>\n      <td>69</td>\n      <td>Greater than 45</td>\n      <td>Other</td>\n      <td>...</td>\n      <td>1</td>\n      <td>Low</td>\n      <td>2013-08-14</td>\n      <td>2014-07-07</td>\n      <td>2014-07-14</td>\n      <td>0</td>\n      <td>0</td>\n      <td>327</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>kevon dixon</td>\n      <td>kevon</td>\n      <td>dixon</td>\n      <td>2013-01-27</td>\n      <td>Male</td>\n      <td>1982-01-22</td>\n      <td>34</td>\n      <td>25 - 45</td>\n      <td>African-American</td>\n      <td>...</td>\n      <td>1</td>\n      <td>Low</td>\n      <td>2013-01-27</td>\n      <td>2013-01-26</td>\n      <td>2013-02-05</td>\n      <td>0</td>\n      <td>9</td>\n      <td>159</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>ed philo</td>\n      <td>ed</td>\n      <td>philo</td>\n      <td>2013-04-14</td>\n      <td>Male</td>\n      <td>1991-05-14</td>\n      <td>24</td>\n      <td>Less than 25</td>\n      <td>African-American</td>\n      <td>...</td>\n      <td>3</td>\n      <td>Low</td>\n      <td>2013-04-14</td>\n      <td>2013-06-16</td>\n      <td>2013-06-16</td>\n      <td>4</td>\n      <td>0</td>\n      <td>63</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>marcu brown</td>\n      <td>marcu</td>\n      <td>brown</td>\n      <td>2013-01-13</td>\n      <td>Male</td>\n      <td>1993-01-21</td>\n      <td>23</td>\n      <td>Less than 25</td>\n      <td>African-American</td>\n      <td>...</td>\n      <td>6</td>\n      <td>Medium</td>\n      <td>2013-01-13</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1174</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>bouthy pierrelouis</td>\n      <td>bouthy</td>\n      <td>pierrelouis</td>\n      <td>2013-03-26</td>\n      <td>Male</td>\n      <td>1973-01-22</td>\n      <td>43</td>\n      <td>25 - 45</td>\n      <td>Other</td>\n      <td>...</td>\n      <td>1</td>\n      <td>Low</td>\n      <td>2013-03-26</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1102</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 53 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "raw_data = pd.read_csv('compas-scores-two-years.csv')\n",
    "# print a list of variable names\n",
    "print(raw_data.columns)\n",
    "# look at the first 5 rows \n",
    "raw_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc0cb9b-f285-4da2-8c5e-bd3af43ee4d7",
   "metadata": {},
   "source": [
    "The data set includes 53 variables. There are different types of information. Some variables\n",
    "* personal data (e.g., name, first name (\"first\"), last name (\"last\")) \n",
    "* demographic data (i.e., sex, age, age category (\"age_cat\"), and race)\n",
    "* related to the person's history of commited offenses (e.g., juvenile felony count (\"juv_fel_count\"), juvenile misdemeanor count (\"juv_misd_count\"), and prior offenses count (\"priors-count\"))\n",
    "* related to the charge against the person (e.g., charge offense date (\"c_offense_date\"), charge arrest date (\"c_arrest_date\"), charge degree (\"c_charge_degree\"), and description of charge (\"c_charge_desc\"))\n",
    "* recidivism scores assigned by the COMPAS algorithm (e.g., \"decile_score\", \"score_text\", \"v_decile_score\", \"v_score_text\")\n",
    "* related to an actual recidivism charge (e.g., degree of recidivism charge (\"r_charge_degree\"), data of recidivism offense (\"r_offense_date\"), description of recidivism charge (\"r_charge_desc\"))\n",
    "* related to an actual violent recidivism charge (e.g., degree of violent recidivism charge (\"vr_charge_degree\"), data of violent recidivism offense (\"vr_offense_date\"), description of violent recidivism charge (\"vr_charge_desc\")).\n",
    "\n",
    "### Part 1, Step 2: Select features and response variables\n",
    "\n",
    "The ProPublica article was assessing bias in the COMPAS scores. Here, you will ignore the COMPAS scores and instead explore the challenges of predicting recidivism based on the survey data. What variables seem like sensible predictors? What variables would be sensible outcome variables? The code in the cell below selects some numerical and categorical variables for you to include in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2fcba38-1f99-4723-96d2-ddf46ac7f7da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:37:55.283166Z",
     "start_time": "2024-07-24T14:37:55.280380Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select features and response variables\n",
    "\n",
    "# Features by type\n",
    "numerical_features = ['juv_misd_count', 'juv_other_count', 'juv_fel_count', \n",
    "    'priors_count', 'age']\n",
    "binary_categorical_features = ['sex', 'c_charge_degree']\n",
    "other_categorical_features = ['race']\n",
    "all_features = binary_categorical_features + other_categorical_features + numerical_features\n",
    "\n",
    "# Possible esponse variables\n",
    "response_variables = ['is_recid', 'is_violent_recid', 'two_year_recid']\n",
    "\n",
    "# Variables that are used for data cleaning\n",
    "check_variables = ['days_b_screening_arrest']\n",
    "\n",
    "\n",
    "# looking at the above variables, it would seem that the numerical features, such as the priors count would be good predictors. It also seems like categories such as race could introduce a lot of bias into the model. Age could also be a senseible predictor and the degree of their charge. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b0cbaa-0a7b-46bf-a6d6-25f7c13fc1c7",
   "metadata": {},
   "source": [
    "ProPublica filtered some observations (i.e., rows in the data frame). See their explanation below. Let's follow their procedure.\n",
    "\n",
    "\n",
    "> There are a number of reasons remove rows because of missing data:\n",
    ">\n",
    "> * If the charge date of a defendants Compas scored crime was not within 30 days from when the person was arrested, we assume that because of data quality reasons, that we do not have the right offense.\n",
    "> * We coded the recidivist flag -- is_recid -- to be -1 if we could not find a compas case at all.\n",
    "> * In a similar vein, ordinary traffic offenses -- those with a c_charge_degree of 'O' -- will not result in Jail time are removed (only two of them).\n",
    "> * We filtered the underlying data from Broward county to include only those rows representing people who had either recidivated in two years, or had at least two years outside of a correctional facility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e11c07da-04e6-42be-b407-31569d56fbcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:37:55.293081Z",
     "start_time": "2024-07-24T14:37:55.286696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe has 6172 rows and 11 columns.\n"
     ]
    }
   ],
   "source": [
    "# Subselect data\n",
    "df = raw_data[all_features+response_variables+check_variables]\n",
    "\n",
    "# Apply filters\n",
    "df = df[(df['days_b_screening_arrest'] <= 30) & \n",
    "        (df['days_b_screening_arrest'] >= -30) & \n",
    "        (df['is_recid'] != -1) & \n",
    "        (df['c_charge_degree'] != 'O')]\n",
    "\n",
    "df = df[all_features+response_variables]\n",
    "print('Dataframe has {} rows and {} columns.'.format(df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c24fa48-c751-4786-b0d6-54a25be38350",
   "metadata": {},
   "source": [
    "### Part 1, Step 3: Construct numerical coding for categorical features\n",
    "\n",
    "Some of these features in the subselected data are not numerical, so you will need to replace some categorical values with zeros and ones. Your features will include \"race\", which was surveyed as a one categorical variable with more than two categories. You will use [1-hot encoding](https://en.wikipedia.org/wiki/One-hot) to include this feature in your data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0fdd6d1-8ead-41ff-a9e6-6ce9149fc545",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:37:55.338585Z",
     "start_time": "2024-07-24T14:37:55.294184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replace Female with 0.\n",
      "Replace Male with 1.\n",
      "Replace F with 0.\n",
      "Replace M with 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rs/5jvwzr1d1klbwx2x36gq7bf00000gn/T/ipykernel_22987/1518869326.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(value, new_value)\n"
     ]
    }
   ],
   "source": [
    "# Code binary features as 0 and 1\n",
    "for x in binary_categorical_features:\n",
    "    for new_value, value in enumerate(set(df[x])):\n",
    "        print(\"Replace {} with {}.\".format(value, new_value))\n",
    "        df = df.replace(value, new_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f026e54b-d977-4ee3-93b8-d47703af9213",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:37:55.339497Z",
     "start_time": "2024-07-24T14:37:55.314110Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    sex  c_charge_degree              race  race_is_Caucasian  \\\n0     1                0             Other              False   \n1     1                0  African-American              False   \n2     1                0  African-American              False   \n5     1                1             Other              False   \n6     1                0         Caucasian               True   \n7     1                0             Other              False   \n8     0                1         Caucasian               True   \n10    1                0         Caucasian               True   \n11    1                1  African-American              False   \n12    0                1         Caucasian               True   \n\n    race_is_Native American  race_is_Asian  race_is_Hispanic  race_is_Other  \\\n0                     False          False             False           True   \n1                     False          False             False          False   \n2                     False          False             False          False   \n5                     False          False             False           True   \n6                     False          False             False          False   \n7                     False          False             False           True   \n8                     False          False             False          False   \n10                    False          False             False          False   \n11                    False          False             False          False   \n12                    False          False             False          False   \n\n    race_is_African-American  juv_misd_count  juv_other_count  juv_fel_count  \\\n0                      False               0                0              0   \n1                       True               0                0              0   \n2                       True               0                1              0   \n5                      False               0                0              0   \n6                      False               0                0              0   \n7                      False               0                0              0   \n8                      False               0                0              0   \n10                     False               0                0              0   \n11                      True               0                0              0   \n12                     False               0                0              0   \n\n    priors_count  age  is_recid  is_violent_recid  two_year_recid  \n0              0   69         0                 0               0  \n1              0   34         1                 1               1  \n2              4   24         1                 0               1  \n5              0   44         0                 0               0  \n6             14   41         1                 0               1  \n7              3   43         0                 0               0  \n8              0   39         0                 0               0  \n10             0   27         0                 0               0  \n11             3   23         1                 0               1  \n12             0   37         0                 0               0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sex</th>\n      <th>c_charge_degree</th>\n      <th>race</th>\n      <th>race_is_Caucasian</th>\n      <th>race_is_Native American</th>\n      <th>race_is_Asian</th>\n      <th>race_is_Hispanic</th>\n      <th>race_is_Other</th>\n      <th>race_is_African-American</th>\n      <th>juv_misd_count</th>\n      <th>juv_other_count</th>\n      <th>juv_fel_count</th>\n      <th>priors_count</th>\n      <th>age</th>\n      <th>is_recid</th>\n      <th>is_violent_recid</th>\n      <th>two_year_recid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>Other</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>69</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>African-American</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>34</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>African-American</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n      <td>24</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Other</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>44</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>0</td>\n      <td>Caucasian</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>14</td>\n      <td>41</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>0</td>\n      <td>Other</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>43</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>1</td>\n      <td>Caucasian</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>39</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1</td>\n      <td>0</td>\n      <td>Caucasian</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>27</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1</td>\n      <td>1</td>\n      <td>African-American</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>23</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0</td>\n      <td>1</td>\n      <td>Caucasian</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>37</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use 1-hot encoding for other categorical variables\n",
    "one_hot_features = []\n",
    "for x in other_categorical_features:\n",
    "    for new_feature, value in enumerate(set(df[x])):\n",
    "        feature_name = \"{}_is_{}\".format(x,value)\n",
    "        df.insert(3, feature_name, df[x]==value)\n",
    "        one_hot_features += [feature_name]\n",
    "\n",
    "# Check what the data frame looks like now\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b3c0b-4322-4ec6-80c8-da10665dc72b",
   "metadata": {},
   "source": [
    "### Part 1, Step 4: Split the data\n",
    "\n",
    "Let's collect the features in one data frame and the responses in another data frame. After that, you will set a small portion of the data set aside for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9ea639f-feff-4bf2-ae40-4bfb6714cf8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:37:55.374193Z",
     "start_time": "2024-07-24T14:37:55.318026Z"
    }
   },
   "outputs": [],
   "source": [
    "# list of features\n",
    "features = numerical_features + binary_categorical_features + one_hot_features\n",
    "\n",
    "# features data frame\n",
    "X = df[features]\n",
    "\n",
    "# responses data frame\n",
    "Y = df[response_variables]\n",
    "\n",
    "# Split the data into a training set containing 90% of the data\n",
    "# and test set containing 10% of the data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0556fa-cd30-439d-b671-80e165cfa5d2",
   "metadata": {},
   "source": [
    "# Part 2: Train and validate a decision tree\n",
    "\n",
    ">In this part, you will fit a decision tree to your data. You will examine the effect of tuning the complexity of the tree via the \"maximum number of leaves\" parameter and use 5-fold cross-validation to find an optimal value.\n",
    ">\n",
    ">This part includes three steps:\n",
    ">\n",
    ">1. Fit a decision tree on the training data\n",
    ">2. Tune the parameter \"maximum number of leaves\"\n",
    ">3. Calculate the selected model's test performance\n",
    "\n",
    "### Part 2, Step 1: Fit a decision tree on the training data\n",
    "\n",
    "Start by fitting a decision tree to your training data. Assess its training accuracy and its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11c2b97d-2348-4e7b-b008-92b99665e9c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:37:55.401110Z",
     "start_time": "2024-07-24T14:37:55.326470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained decision tree with 2043 leaves and training accuracy 0.50.\n"
     ]
    }
   ],
   "source": [
    "# Create a model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "    \n",
    "# Fit model to training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "'''ADD SOME CODE HERE'''\n",
    "\n",
    "# Evaluate training accuracy\n",
    "accuracy= model.score(X_test, y_test)\n",
    "'''ADD SOME CODE HERE'''\n",
    "\n",
    "# Check size of decision tree\n",
    "num_leaves = model.get_n_leaves()\n",
    "\n",
    "# Report results\n",
    "print('Trained decision tree with {} leaves and training accuracy {:.2f}.'.format(num_leaves, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881e0628-df55-49f3-a00a-123ca44baa98",
   "metadata": {},
   "source": [
    "Your tree has a good training accuracy for the standards of tabular data prediction problems, but its size is enormous! It has so many leaves, that on average every 3 to 4 training observations get a leaf to themselves. It is very probable that this tree is overfitting.\n",
    "\n",
    "### Part 2, Step 2: Tune the parameter \"maximum number of leaves\"\n",
    "\n",
    "Let's try to constrain the complexity of a decision tree during training by setting a value for the argument ``maximum number of leaves``. You can use the sci-kit learn's `cross_val_score` function to quickly assess the out-of-sample performance of trees of varying complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0668136-b649-4b42-be6b-e360ff7d2f22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:37:56.303943Z",
     "start_time": "2024-07-24T14:37:55.347882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaves\tMean accuracy\n",
      "---------------------\n",
      "101\t0.564\n",
      "201\t0.563\n",
      "301\t0.560\n",
      "401\t0.547\n",
      "501\t0.542\n",
      "601\t0.539\n",
      "701\t0.536\n",
      "801\t0.529\n",
      "901\t0.530\n",
      "1001\t0.526\n",
      "1101\t0.519\n",
      "1201\t0.518\n",
      "1301\t0.508\n",
      "1401\t0.506\n",
      "1501\t0.503\n",
      "1601\t0.508\n",
      "1701\t0.508\n"
     ]
    }
   ],
   "source": [
    "# Perform 5-fold cross-validation for different tree sizes\n",
    "\n",
    "print('Leaves\\tMean accuracy')\n",
    "print('---------------------')\n",
    "for num_leaves in range(1,1800,100):\n",
    "\n",
    "    # Trees must have at least 2 leaves\n",
    "    if num_leaves >= 2:\n",
    "\n",
    "        # construct a classifier with a limit on its number of leaves\n",
    "        model = DecisionTreeClassifier(max_leaf_nodes=num_leaves)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Get validation accuracy via 5-fold cross-validation\n",
    "        scores = cross_val_score(model, X_train, y_train)\n",
    "    \n",
    "    \n",
    "        print(\"{}\\t{:.3f}\".format(num_leaves,scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a46824a-f7f5-4408-94fb-8c463c5a7e0f",
   "metadata": {},
   "source": [
    "Adjust the range of values for `max_leaf_nodes` in the cell above, to identify the best value.\n",
    "\n",
    "Looking at the possible ranges, the best accuracy is .593 with 22 nodes \n",
    "\n",
    "### Part 2, Step 3: Calculate the selected model's test performance\n",
    "\n",
    "Train a decision tree using your selected value of `max_leaf_nodes` on the full training set. Assess its accuracy on your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "502349cb-63ee-4146-8c5e-f58b26241165",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:37:56.316568Z",
     "start_time": "2024-07-24T14:37:56.307643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained decision tree with 22 leaves and test accuracy 0.56.\n"
     ]
    }
   ],
   "source": [
    "# Create a model\n",
    "\n",
    "model = DecisionTreeClassifier(max_leaf_nodes=22)\n",
    "    \n",
    "# Fit model to training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate training accuracy\n",
    "score = model.score(X_test, y_test)\n",
    "\n",
    "\n",
    "# Check size of decision tree\n",
    "size = model.get_n_leaves()\n",
    "\n",
    "\n",
    "\n",
    "# Report results\n",
    "print('Trained decision tree with {} leaves and test accuracy {:.2f}.'.format(size, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415c6b58-4f9e-4d22-b9c0-914b22aea0f9",
   "metadata": {},
   "source": [
    "# Part 3: Auditing a decision tree for demographic biases\n",
    "\n",
    ">Your training data includes several demographic variables (i.e., age, sex, race). A crude way to assess whether a model has some demographic bias is to remove the corresponding variables from your training data and explore how that removal affects your model's performance. Decision trees have the advantage of being interpretable machine learning models. By going through the decision nodes (i.e., branching points), you can \"open the black box and look inside\". Specifically, you can assess how each feature is used in the decision making process.\n",
    ">\n",
    ">This part includes two steps:\n",
    ">\n",
    ">1. Check for racial bias via performance assessment\n",
    ">2. Check for racial bias via decision rules\n",
    "  \n",
    "### Part 3, Step 2: Check for racial bias via performance assessment\n",
    "A simple approach to identifying demographic biases in machine learning is the following: (i) Train and validate the model on the full training set, (ii) train and validate the model on a subset of training variables that excludes the variables related to a potential demographic bias, (iii) compare the results. \n",
    "\n",
    "You have noticed that the validation accuracy of your model can vary for different holdout set selections. To account for these variations, you are going to compare the mean validation accuracy over 100 trees. (You have completed (i) in the previous cell already. Continue now with (ii).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25f5e4e1-5487-4220-afcc-eeb1e228c8ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:37:56.340638Z",
     "start_time": "2024-07-24T14:37:56.318569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained decision tree with 39 leaves and test accuracy 0.67.\n"
     ]
    }
   ],
   "source": [
    "# Create subset of training data without information on race. \n",
    "# (The information on race was encoded in the one-hot features.)\n",
    "remaining_features = [v for v in X.columns if v not in one_hot_features]\n",
    "X_train_sub = X_train[remaining_features]\n",
    "X_test_sub = X_test[remaining_features]\n",
    "\n",
    "# Create a model\n",
    "dtc = DecisionTreeClassifier(max_leaf_nodes=39)\n",
    "    \n",
    "# Fit model to training data\n",
    "dtc.fit(X_train_sub, y_train['two_year_recid'])\n",
    "\n",
    "# Evaluate training accuracy\n",
    "y_pred = dtc.predict(X_test_sub)\n",
    "accuracy = (y_pred == y_test['two_year_recid']).mean()\n",
    "\n",
    "# Check size of decision tree\n",
    "num_leaves = dtc.get_n_leaves()\n",
    "\n",
    "# Report results\n",
    "print('Trained decision tree with {} leaves and test accuracy {:.2f}.'.format(num_leaves, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e10fce-64ad-4f3c-95db-9e31a5e33568",
   "metadata": {},
   "source": [
    "Comparing the mean accuracy values on the all features versus the subselected feature set, what do you conclude about the importance of racial information in this classification problem?\n",
    "\n",
    "Here we can see that the training accuracy goes up by almost 10% when we get rid of race as a factor. This shows that the model holds a lot of racial bias when we include race as a parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18654ba4-834b-49d8-b110-712844d5bf62",
   "metadata": {},
   "source": [
    "### Part 2, Step 3: Check for racial bias via decision rules\n",
    "The interpretability of decision trees allows for an alternative approach to detecting racial bias. You can simply look at the decision rules. Use the scit-kit learn's function `export_text` to get your decision tree in text format. Compare the decision rules of the your tree with all features and your tree fitted on the subset without racial information. Do you find any indication of racial bias in the decision rules of the first tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "212a88fb-f48a-4081-87df-eb1ce5b69bc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:37:56.342270Z",
     "start_time": "2024-07-24T14:37:56.328569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'|--- feature_3 <= 2.50\\n|   |--- feature_4 <= 22.50\\n|   |   |--- feature_4 <= 20.50\\n|   |   |   |--- feature_3 <= 0.50\\n|   |   |   |   |--- feature_4 <= 19.50\\n|   |   |   |   |   |--- class: 1\\n|   |   |   |   |--- feature_4 >  19.50\\n|   |   |   |   |   |--- class: 1\\n|   |   |   |--- feature_3 >  0.50\\n|   |   |   |   |--- class: 1\\n|   |   |--- feature_4 >  20.50\\n|   |   |   |--- feature_5 <= 0.50\\n|   |   |   |   |--- feature_4 <= 21.50\\n|   |   |   |   |   |--- class: 0\\n|   |   |   |   |--- feature_4 >  21.50\\n|   |   |   |   |   |--- class: 0\\n|   |   |   |--- feature_5 >  0.50\\n|   |   |   |   |--- feature_3 <= 1.50\\n|   |   |   |   |   |--- class: 1\\n|   |   |   |   |--- feature_3 >  1.50\\n|   |   |   |   |   |--- class: 1\\n|   |--- feature_4 >  22.50\\n|   |   |--- feature_3 <= 0.50\\n|   |   |   |--- feature_4 <= 35.50\\n|   |   |   |   |--- class: 0\\n|   |   |   |--- feature_4 >  35.50\\n|   |   |   |   |--- feature_4 <= 52.50\\n|   |   |   |   |   |--- feature_4 <= 50.50\\n|   |   |   |   |   |   |--- class: 0\\n|   |   |   |   |   |--- feature_4 >  50.50\\n|   |   |   |   |   |   |--- class: 0\\n|   |   |   |   |--- feature_4 >  52.50\\n|   |   |   |   |   |--- class: 0\\n|   |   |--- feature_3 >  0.50\\n|   |   |   |--- feature_4 <= 32.50\\n|   |   |   |   |--- feature_3 <= 1.50\\n|   |   |   |   |   |--- feature_0 <= 0.50\\n|   |   |   |   |   |   |--- class: 0\\n|   |   |   |   |   |--- feature_0 >  0.50\\n|   |   |   |   |   |   |--- class: 0\\n|   |   |   |   |--- feature_3 >  1.50\\n|   |   |   |   |   |--- feature_4 <= 24.50\\n|   |   |   |   |   |   |--- class: 1\\n|   |   |   |   |   |--- feature_4 >  24.50\\n|   |   |   |   |   |   |--- feature_5 <= 0.50\\n|   |   |   |   |   |   |   |--- class: 1\\n|   |   |   |   |   |   |--- feature_5 >  0.50\\n|   |   |   |   |   |   |   |--- feature_0 <= 0.50\\n|   |   |   |   |   |   |   |   |--- class: 0\\n|   |   |   |   |   |   |   |--- feature_0 >  0.50\\n|   |   |   |   |   |   |   |   |--- class: 1\\n|   |   |   |--- feature_4 >  32.50\\n|   |   |   |   |--- class: 0\\n|--- feature_3 >  2.50\\n|   |--- feature_4 <= 33.50\\n|   |   |--- feature_3 <= 7.50\\n|   |   |   |--- feature_4 <= 27.50\\n|   |   |   |   |--- feature_6 <= 0.50\\n|   |   |   |   |   |--- class: 1\\n|   |   |   |   |--- feature_6 >  0.50\\n|   |   |   |   |   |--- class: 1\\n|   |   |   |--- feature_4 >  27.50\\n|   |   |   |   |--- feature_3 <= 5.50\\n|   |   |   |   |   |--- feature_5 <= 0.50\\n|   |   |   |   |   |   |--- class: 0\\n|   |   |   |   |   |--- feature_5 >  0.50\\n|   |   |   |   |   |   |--- class: 1\\n|   |   |   |   |--- feature_3 >  5.50\\n|   |   |   |   |   |--- feature_2 <= 1.50\\n|   |   |   |   |   |   |--- feature_4 <= 29.50\\n|   |   |   |   |   |   |   |--- class: 1\\n|   |   |   |   |   |   |--- feature_4 >  29.50\\n|   |   |   |   |   |   |   |--- class: 1\\n|   |   |   |   |   |--- feature_2 >  1.50\\n|   |   |   |   |   |   |--- class: 0\\n|   |   |--- feature_3 >  7.50\\n|   |   |   |--- class: 1\\n|   |--- feature_4 >  33.50\\n|   |   |--- feature_3 <= 6.50\\n|   |   |   |--- feature_4 <= 55.50\\n|   |   |   |   |--- class: 0\\n|   |   |   |--- feature_4 >  55.50\\n|   |   |   |   |--- class: 0\\n|   |   |--- feature_3 >  6.50\\n|   |   |   |--- feature_3 <= 15.50\\n|   |   |   |   |--- feature_1 <= 0.50\\n|   |   |   |   |   |--- feature_5 <= 0.50\\n|   |   |   |   |   |   |--- feature_2 <= 0.50\\n|   |   |   |   |   |   |   |--- feature_4 <= 42.50\\n|   |   |   |   |   |   |   |   |--- class: 1\\n|   |   |   |   |   |   |   |--- feature_4 >  42.50\\n|   |   |   |   |   |   |   |   |--- class: 1\\n|   |   |   |   |   |   |--- feature_2 >  0.50\\n|   |   |   |   |   |   |   |--- class: 0\\n|   |   |   |   |   |--- feature_5 >  0.50\\n|   |   |   |   |   |   |--- feature_0 <= 2.50\\n|   |   |   |   |   |   |   |--- class: 1\\n|   |   |   |   |   |   |--- feature_0 >  2.50\\n|   |   |   |   |   |   |   |--- class: 0\\n|   |   |   |   |--- feature_1 >  0.50\\n|   |   |   |   |   |--- class: 1\\n|   |   |   |--- feature_3 >  15.50\\n|   |   |   |   |--- feature_3 <= 21.50\\n|   |   |   |   |   |--- feature_4 <= 39.50\\n|   |   |   |   |   |   |--- class: 1\\n|   |   |   |   |   |--- feature_4 >  39.50\\n|   |   |   |   |   |   |--- feature_3 <= 18.50\\n|   |   |   |   |   |   |   |--- feature_2 <= 0.50\\n|   |   |   |   |   |   |   |   |--- class: 1\\n|   |   |   |   |   |   |   |--- feature_2 >  0.50\\n|   |   |   |   |   |   |   |   |--- class: 0\\n|   |   |   |   |   |   |--- feature_3 >  18.50\\n|   |   |   |   |   |   |   |--- class: 1\\n|   |   |   |   |--- feature_3 >  21.50\\n|   |   |   |   |   |--- class: 1\\n'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''ADD SOME CODE HERE'''\n",
    "\n",
    "export_text(dtc)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking at this tree, we can definitely see intstances of racial bias. Looking at the table, frature 3 is the race column, meaning that all of the times feature 3 comes up in the above graph, it is considering rance (which is a lot). This also makes sense because criminal record data is skewed towards having more people of color included, which would included prediction bias."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b89f3c2f9323f1b3"
  },
  {
   "cell_type": "markdown",
   "id": "1e82dfe8-9fd7-44f9-aca4-8e33f9203798",
   "metadata": {},
   "source": [
    "# Part 4: Comparison to other linear classifiers\n",
    "\n",
    ">For some types of data, decision trees tend to achieve lower prediction accuracies In this part, you will train and tune several classifiers on the COMPAS data. You will then compare their performance on your test set.\n",
    ">\n",
    ">This part includes three steps:\n",
    ">\n",
    ">1. Fit LDA and logistic regression\n",
    ">2. Tune and fit ensemble methods\n",
    ">3. Tune and fit SVC\n",
    ">4. Compare test accuracy of all your models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e97edaf2-04da-4f12-aa8f-75847bd6e3ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:37:56.468694Z",
     "start_time": "2024-07-24T14:37:56.334627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6699029126213593\n",
      "0.6601941747572816\n"
     ]
    }
   ],
   "source": [
    "# first doing LDA and logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "logReg = LogisticRegression(max_iter=200)\n",
    "logReg.fit(X_train, y_train['two_year_recid'])\n",
    "y_pred = logReg.predict(X_test)\n",
    "accuracy = (y_pred == y_test['two_year_recid']).mean()\n",
    "print(accuracy)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train['two_year_recid'])\n",
    "y_pred = lda.predict(X_test)\n",
    "accuracy = (y_pred == y_test['two_year_recid']).mean()\n",
    "print(accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6343042071197411\n",
      "0.627831715210356\n",
      "0.6747572815533981\n",
      "0.627831715210356\n",
      "0.6359223300970874\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# tune and fit the ensemble methods \n",
    "\n",
    "# RFC \n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train['two_year_recid'])\n",
    "y_pred = rfc.predict(X_test)\n",
    "accuracy = (y_pred == y_test['two_year_recid']).mean()\n",
    "print(accuracy)\n",
    "\n",
    "bc = BaggingClassifier()\n",
    "bc.fit(X_train, y_train['two_year_recid'])\n",
    "bc.predict(X_test)\n",
    "y_pred = bc.predict(X_test)\n",
    "accuracy = (y_pred == y_test['two_year_recid']).mean()\n",
    "print(accuracy)\n",
    "\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(X_train, y_train['two_year_recid'])\n",
    "bc.predict(X_test)\n",
    "y_pred = gbc.predict(X_test)\n",
    "accuracy = (y_pred == y_test['two_year_recid']).mean()\n",
    "print(accuracy)\n",
    "\n",
    "\n",
    "# running on the model without racial data:\n",
    "# RFC \n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_sub, y_train['two_year_recid'])\n",
    "y_pred = rfc.predict(X_test_sub)\n",
    "accuracy = (y_pred == y_test['two_year_recid']).mean()\n",
    "print(accuracy)\n",
    "\n",
    "bc = BaggingClassifier()\n",
    "bc.fit(X_train_sub, y_train['two_year_recid'])\n",
    "y_pred = bc.predict(X_test_sub)\n",
    "accuracy = (y_pred == y_test['two_year_recid']).mean()\n",
    "print(accuracy)\n",
    "\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(X_train_sub, y_train['two_year_recid'])\n",
    "y_pred = gbc.predict(X_test_sub)\n",
    "accuracy = (y_pred == y_test['two_year_recid']).mean()\n",
    "print(accuracy)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T14:40:16.619749Z",
     "start_time": "2024-07-24T14:40:15.459572Z"
    }
   },
   "id": "1e5a2b55ed9ae523"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.558252427184466\n"
     ]
    }
   ],
   "source": [
    "# tune and fit SVC\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train['two_year_recid'])\n",
    "y_pred = svc.predict(X_test)\n",
    "accuracy = (y_pred == y_test['two_year_recid']).mean()\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T14:45:53.987219Z",
     "start_time": "2024-07-24T14:45:52.918429Z"
    }
   },
   "id": "ff22127bead3f8d7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Overall, the performance across all models was best in the straight decision tree with a modified number of nodes and eliminating racial bias. The SVC did not perform very well either. It is also important to note that no accuracy got above around a 70%, which is not very high. This is important to keep in mind as people start to make decisions based off of machine learning models. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be6ed36e2a72088d"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T14:37:57.131974Z",
     "start_time": "2024-07-24T14:37:57.128122Z"
    }
   },
   "id": "a639bb6793b192c9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
